\documentclass{beamer}
%% fink install texlive
\usetheme{Warsaw}

\usepackage{amsmath,amssymb}
\usepackage{stmaryrd}
\usepackage{graphicx}
%\usepackage{haskell}
\usepackage{code}
%\usepackage{proof}
\usepackage{theorem}
\usepackage{pstricks} 
\usepackage{listings}
\usepackage{pgf-pie} % from https://code.google.com/p/pgf-pie/


\include{macros-ms}

\newcommand{\beb}{\begin{exampleblock}}
\newcommand{\eeb}{\end{exampleblock}}


\newcommand{\rev}[1]{{#1}^{\mbox{\scriptsize r}}}
\newcommand{\lquo}{\backslash}
\newcommand{\rquo}{/}
\newcommand{\diff}{-}
\newcommand{\isect}{\cap}
\newcommand{\mymid}{~\redtxt{\mid}~}

\newcommand{\ignore}[1]{}

\newcommand{\magtxt}[1]{{\magenta #1}}
\newcommand{\redtxt}[1]{{\red #1}}
\newcommand{\bluetxt}[1]{{\blue #1}}
\newcommand{\greytxt}[1]{{\gray #1}}
\newcommand{\mmleq}{\leq}
\newcommand{\pow}{\^{}}
\newcommand{\venv}{\Delta}
\newcommand{\mleq}{\mbox{\tt leq}}
\newcommand{\mas}{\mbox{\tt as}}
\newcommand{\subt}{<\!:}

\newcommand{\Nturns}{\, \vdash_{\mbox{\tiny lnf}} \,}

\newenvironment{ttprog}{\begin{trivlist}\item \tt
        \begin{tabbing}}{\end{tabbing}\end{trivlist}}


\newenvironment{grammar}{%
         \begin{center} \small%
         $\begin{array}{rcll}
         }{%
         \end{array}$\end{center}\ignorespaces%
         }

\newcommand{\rem}[1]{}

\begin{document}
\logo{
\includegraphics[height=0.7cm]{./logo/nyp-logo-int.png} 
\hspace{260pt}
\vspace{230pt}

}
\lstset{language=python}

\title{ITD302 \\ Massively Parallel Computing for Big Data \\
  Introduction to Spark} 
\author{ \white 
 Copyright \copyright~ 2015 Nanyang Polytechnic. All Rights Reserved
 % Kenny Zhuo Ming Lu 
}
\institute{
\normalsize 
School of Information Technology \\ Nanyang Polytechnic
}
\date{\today} 


%\bibliographystyle{plainnat}

\frame{\titlepage} 

%\frame{\frametitle{Table of contents}\tableofcontents} 


\begin{frame}
\frametitle{Learning Outcomes}

\begin{itemize}
  \item Describe and explain the concepts, features and
    functionalities of Spark.
  \item Develop real-time scalable in memory data transformation and
    analytics application using Spark.
\end{itemize}
\end{frame}


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\begin{frame}
\frametitle{What is Spark?}
\begin{itemize}
\item A distrbute cluster computing system favoring in-memory computation. 
\item It was developed intially for batch processing computation like MapReduce
\end{itemize}
\end{frame}


%-------------------------------------------------------------------
%-------------------------------------------------------------------
\begin{frame}
\frametitle{Why Spark?}
What's wrong with MapReduce?
\begin{itemize}
 \item  it was designed for moderate CPU and low memory systems. 
 \item  it relies on disk I/O operations at each intermediate steps. 
 \item Its performance is capped by the disk I/O performance, and symmetric distribution of the
Reduce jobs.
\end{itemize}
Spark comes in assuming our machines are in general more powerful, and
RAMs are cheaper. 
\begin{itemize}
\item it favors in memory computations. Data are loaded from disk and
  stay in memory as long as possible.
\item it uses resillent distributed datasets (RDD) as the abstract data collections.
\item it performs better than MapReduce if we have sufficient RAM in
  the cluster.
\end{itemize}
\end{frame}



%-------------------------------------------------------------------
%-------------------------------------------------------------------
\begin{frame}
\frametitle{Spark vs Hadoop MapReduce}

\begin{tabular}{|c|c|c|} \hline
 & Spark & HD MR \\ \hline \hline 
Objectives & Data volume & Data volume \\ \hline 
Type of parallelism & Data & Data \\ \hline 
Task life-time  & Fixed & Fixed \\  \hline 
Input  & Block and batch data  & Block and batch data \\ \hline 
Roles & Master and workers & Master and slaves \\  \hline 
Components & Driver programs & Mappers and reducers \\ \hline 
Task  & Jobs & Jobs \\ \hline
\end{tabular}
\end{frame}
%-------------------------------------------------------------------
%-------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{Spark Architecture}

\begin{figure}[!htb]
\centering
\includegraphics[scale=0.5]{pic/spark.png}
\end{figure}

\end{frame}

%-------------------------------------------------------------------
%-------------------------------------------------------------------
\begin{frame}[fragile]
\frametitle{A use case}

\beb{Wordcount in Scala} 
\begin{code}
val lines = sc.textFile("hdfs://127.0.0.1:9000/input/")
val counts = lines.flatMap(line => line.split(" "))
    .map(word => (word, 1))
    .reduceByKey(_ + _)
counts.saveAsTextFile("hdfs://127.0.0.1:9000/output/")
\end{code} \eeb

\beb{Wordcount in Python}
\begin{code}
lines = sc.textFile("hdfs://127.0.0.1:9000/input/")
counts = lines.flatMap(lambda x: x.split(' ')) \
              .map(lambda x: (x, 1)) \
              .reduceByKey(add)
couts.saveAsTextFile("hdfs://127.0.0.1:9000/output/")
\end{code} \eeb
{\tt sc} denotes SparkContext
\end{frame}



%-------------------------------------------------------------------
%-------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{How to run it?}

First start the cluster
\begin{code}
$ /opt/spark-1.4.1-bin-hadoop2.6/sbin/start-all.sh
\end{code}
%$
\end{frame}
%-------------------------------------------------------------------
%-------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Run it in the REPL}

\beb{}
\begin{code}
$ /opt/spark-1.4.1-bin-hadoop2.6/bin/spark-shell
 scala> :load Wordcount.scala
\end{code}
%$
\eeb
Or we can type the code in line by line. 
\end{frame}


%-------------------------------------------------------------------
%-------------------------------------------------------------------

\begin{frame}[fragile]
\frametitle{Submit to the cluster}

\beb{Scala}
\begin{code}
$ /opt/spark-1.4.1-bin-hadoop2.6/bin/spark-submit
 Wordcount.jar
\end{code}
%$
\eeb

\beb{Python}
\begin{code}
$ /opt/spark-1.4.1-bin-hadoop2.6/bin/spark-submit 
wordcount.py
\end{code}
%$
\eeb

It supports R too.
\end{frame}


%-------------------------------------------------------------------
%-------------------------------------------------------------------




\end{document}
